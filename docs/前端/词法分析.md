# 词法分析

> 作者：zxw
> 
> 最近一次更新于2025/3/23

Lexer类定义于`include/Frontend/Lexer.h`，实现于`src/Frontend/Lexer.cpp`。

Lexer使用的Token类定义于`include/Utils/Token.h`。

## Token

词法单元类`Token`位于`namespace Token`下，数据结构为：

```cpp
class Token {
public:
    // 源程序字符串
    const std::string content;
    // token类型
    const Type type;
    // token所在的行号
    const int line;

    Token(std::string c, const Type t, const int l)
        : content(std::move(c)), type(t), line(l) {}

    [[nodiscard]] std::string to_string() const;
};
```

Type为`namespace Token`下的一个枚举类，标记了每个token属于什么类型：
```cpp
enum class Type {
    // 关键词
    CONST, INT, FLOAT, VOID, IF, ELSE, WHILE, BREAK, CONTINUE, RETURN,
    // 标识符
    IDENTIFIER,
    // 字面量
    INT_CONST, FLOAT_CONST, STRING_CONST,
    // 运算符
    ADD, SUB, NOT, MUL, DIV, MOD,
    LT, GT, LE, GE, EQ, NE, AND, OR,
    // 分隔符
    SEMICOLON, COMMA, ASSIGN,
    LPAREN, RPAREN, LBRACE, RBRACE, LBRACKET, RBRACKET,
    // 结束符
    END_OF_FILE,
    // 未知
    UNKNOWN
};
```

## Lexer

### 接口

Lexer在`Compiler.cpp`中的接口为`tokenize`，使用方法如下：
```cpp
// 从文件中读入源程序
std::ifstream file(options.input_file);
if (!file.is_open()) {
    log_fatal();
}

std::stringstream buffer;
buffer << file.rdbuf();
std::string src_code = buffer.str();
file.close();
Lexer lexer(src_code);
const std::vector<Token::Token> &tokens = lexer.tokenize();
```

Lexer接受`std::string`的源程序内容，返回`std::vector<Token::Token>`，是分割好的Token的列表。

### 实现
#### consume_line_comment

​匹配掉两个 / 字符，之后一直匹配下一个\n前的所有字符。

​其中做越界检查，防止最后一行之后无\n。

#### consume_block_comment

​匹配掉/*字符，之后一直匹配知道遇到\*/ 再将其匹配掉。

​其中做越界检查。

#### consume_ident_or_keyword

​以首字符所在行数为 token 行数，判断是否是标识符中字符。

​之后以 keywords.find 检查字符内容，找到时返回对应关键字的token，否则返回标识符token。

#### consume_number

识别前缀，并按前缀分进制识别数字，区分整数与浮点数。

这里直接转换为十进制整数或浮点数，再转换为字符串。

#### consume_string

首先吸收一个引号，之后若为转义符号，连续吸收两个字符；若为引号则结束，否则继续。

字符串形成 Token 时，字符内容只包含双引号中间内容，不包含双引号内容。

#### consume_operator

先匹配两个字符，匹配不成功：匹配单个字符，否则返回特殊符号 UNKNOWN。

#### tokenize

检查首字符，观察是否需要跳过空白部分或注释。

之后分为四种情况：

+ 字母/`_` ：标识符
+ 数字或`.`：number
+ 双引号：字符串
+ 其他：分隔符或特殊字符

最后加入特殊的 `END_OF_FILE` token。
